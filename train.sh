#!/bin/bash

python3 main.py train \
  --num_encoder_layers=3 \
  --num_decoder_layers=3 \
  --vocab_src_size=-1 \
  --vocab_tgt_size=-1 \
  --pad_src_idx=-1 \
  --pad_tgt_idx=-1 \
  --embedding_size=512 \
  --query_key_size=64 \
  --value_size=64 \
  --num_heads=8 \
  --ffn_hidden_dim=512 \
  --ffn_activation="relu" \
  --use_ffn_bias_1 \
  --use_ffn_bias_2 \
  --dropout_rate=0.1 \
  --max_length=64 \
  --weight_initialization_method="kaiming_uniform" \
  --learning_rate=1e-04 \
  --weight_decay=0.0001 \
  --batch_size=16 \
  --dataset_name="multi30k" \
  --tokenizer_type="bpe" \
  --tokenizer_vocab_size=5000 \
  --tokenizer_min_frequency=2 \
  --epochs=10 \
  --seed=42 \
  --validation_epochs=1 \
  --checkpoint_path="checkpoints" \
  --checkpoint_name="transformer" \
  --checkpoint_steps=10000 \
  --gradient_accumulation_steps=1 \
